{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b68b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainable parameters are the number of, well, trainable elements in your network; neurons that are affected by backpropagation. For example, for the Wx + b operation in each neuron, W and b are trainable – because they are changed by optimizers after backpropagation was applied for gradient computation. Nontrainable ones are e.g. Batch Normalization or when you lock/freeze layers e.g. during transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can apply a Dropout layer to the input vector, in which case it nullifies some of its features; but we can also apply it to a hidden layer, in which case it nullifies some hidden neurons. Dropout layers are important in training CNNs because they prevent overfitting on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560c0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "Usually, for most applications, one hidden layer is enough. Also, the number of neurons in that hidden layer should be between the number of inputs (10 in your example) and the number of outputs (5 in your example). But the best way to choose the number of neurons and hidden layers is experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487df4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "In lower layers of the network, filters may activate when they see edge-like or corner-like regions. Then, in the deeper layers of the network, filters may activate in the presence of high-level features, such as parts of the face, the paw of a dog, the hood of a car, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b92741",
   "metadata": {},
   "outputs": [],
   "source": [
    "A traditional default value for the learning rate is 0.1 or 0.01, and this may represent a good starting point on your problem. A default value of 0.01 typically works for standard multi-layer neural networks but it would be foolish to rely exclusively on this default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c0aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Activation functions are functions used in a neural network to compute the weighted sum of inputs and biases, which is in turn used to decide whether a neuron can be activated or not. It manipulates the presented data and produces an output for the neural network that contains the parameters in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data normalization is the organization of data to appear similar across all records and fields. It increases the cohesion of entry types, leading to cleansing, lead generation, segmentation, and higher quality data. Data Normalization disposes of various anomalies that can make an examination of the information more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image Augmentation is the process of generating new images for training our deep learning model. These new images are generated using the existing training images and hence we don’t have to collect them manually. There are multiple image augmentation techniques and we will discuss some of the common and most widely used ones in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359422a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "If we record the learning at each iteration and plot the learning rate (log) against loss; we will see that as the learning rate increase, there will be a point where the loss stops decreasing and starts to increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec662073",
   "metadata": {},
   "outputs": [],
   "source": [
    "Early stopping requires that you configure your network to be under constrained, meaning that it has more capacity than is required for the problem. When training the network, a larger number of training epochs is used than may normally be required, to give the network plenty of opportunity to fit, then begin to overfit the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7bdcbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
