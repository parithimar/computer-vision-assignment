{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b58535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Inception network was once considered a state-of-the-art deep learning architecture (or model) for solving image recognition and detection problems. It put forward a breakthrough performance on the ImageNet Visual Recognition Challenge (in 2014), which is a reputed platform for benchmarking image recognition and detection algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a5292",
   "metadata": {},
   "outputs": [],
   "source": [
    "The inception module is different from previous architectures such as AlexNet, ZF-Net. In this architecture, there is a fixed convolution size for each layer. In the Inception module 1×1, 3×3, 5×5 convolution and 3×3 max pooling performed in a parallel way at the input and the output of these are stacked together to generated final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5735d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Large numbers of input features can cause poor performance for machine learning algorithms.\n",
    "Dimensionality reduction is a general field of study concerned with reducing the number of input features.\n",
    "Dimensionality reduction methods include feature selection, linear algebra methods, projection methods, and autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbba4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sometimes, most of these features are correlated, and hence redundant. This is where dimensionality reduction algorithms come into play. Dimensionality reduction is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables. It can be divided into feature selection and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "The paper proposes a new type of architecture – GoogLeNet or Inception v1. It is basically a convolutional neural network (CNN) which is 27 layers deep. Below is the model summary: Notice in the above image that there is a layer called inception layer. This is actually the main idea behind the paper’s approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a670dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "The basic architecture of ResNeXt is defined by two rules. First, if the blocks produce same-dimensional spatial maps, they share the same set of hyperparameters, and if at all the spatial map is downsampled by a factor of 2, the width of the block is multiplied by a factor of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "Skip connections in deep architectures, as the name suggests, skip some layer in the neural network and feeds the output of one layer as the input to the next layers (instead of only the next one). As previously explained, using the chain rule, we must keep multiplying terms with the error gradient as we go backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Understanding a residual block is quite easy. In traditional neural networks, each layer feeds into t he next layer. In a network with residual blocks, each layer feeds into the next layer and directly into the layers about 2–3 hops away. That’s it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f40936",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71873250",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transfer learning is a machine learning technique that enables data scientists to benefit from the knowledge gained from a previously used machine learning model for a similar task. This learning takes humans’ ability to transfer their knowledge as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e8a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Fine-tuning, an approach of Transfer Learning, we have a dataset, and we use let's say 90% of it in training. Then we train the same model with the remaining 10%. Usually, we change the learning rate to a smaller one, so it does not have a significant impact on the already adjusted weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2859a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
