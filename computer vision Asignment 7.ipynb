{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b32ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Covariate shift is the change in the distribution of the covariates specifically, that is, the independent variables. This is normally due to changes in state of latent variables, which could be temporal (even changes to the stationarity of a temporal process), or spatial, or less obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944f973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The batch normalization is for layers that can suffer from deleterious drift. The math is simple: find the mean and variance of each component, then apply the standard transformation to convert all values to the corresponding Z-scores: subtract the mean and divide by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "The first layer is the input layer — this is generally not considered a layer of the network as nothing is learnt in this layer. The input layer is built to take in 32x32, and these are the dimensions of images that are passed into the next layer. Those who are familiar with the MNIST dataset will be aware that the MNIST dataset images have the dimensions 28x28. To get the MNIST images dimension to the meet the requirements of the input layer, the 28x28 images are padded.\n",
    "The grayscale images used in the research paper had their pixel values normalized from 0 to 255, to values between -0.1 and 1.175. The reason for normalization is to ensure that the batch of images have a mean of 0 and a standard deviation of 1, the benefits of this is seen in the reduction in the amount of training time. In the image classification with LeNet-5 example below, we’ll be normalizing the pixel values of the images to take on values between 0 to 1.\n",
    "The LeNet-5 architecture utilizes two significant types of layer construct: convolutional layers and subsampling layers.\n",
    "Convolutional layers\n",
    "Sub-sampling layers\n",
    "Within the research paper and the image below, convolutional layers are identified with the ‘Cx’, and subsampling layers are identified with ‘Sx’, where ‘x’ is the sequential position of the layer within the architecture. ‘Fx’ is used to identify fully connected layers. This method of layer identification can be seen in the image above.\n",
    "The official first layer convolutional layer C1 produces as output 6 feature maps, and has a kernel size of 5x5. The kernel/filter is the name given to the window that contains the weight values that are utilized during the convolution of the weight values with the input values. 5x5 is also indicative of the local receptive field size each unit or neuron within a convolutional layer. The dimensions of the six feature maps the first convolution layer produces are 28x28.\n",
    "A subsampling layer ‘S2’ follows the ‘C1’ layer’. The ‘S2’ layer halves the dimension of the feature maps it receives from the previous layer; this is known commonly as downsampling.\n",
    "The ‘S2’ layer also produces 6 feature maps, each one corresponding to the feature maps passed as input from the previous layer. This link contains more information on subsampling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609ce25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet Architecture. AlexNet contains five convolutional layers and three fully connected layers – total of eight layers. AlexNet architecture is shown below: AlexNet Architecture. source. For the first two convolutional layers, each convolutional layers is followed by a Overlapping Max Pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea16798",
   "metadata": {},
   "outputs": [],
   "source": [
    "This problem of very small gradients is known as the vanishing gradient problem. The vanishing gradient problem particularly affects the lower layers of the network and makes them more difficult to train. Similarly, if the gradient associated with a weight becomes extremely large the updates to the weight will also be large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab35367",
   "metadata": {},
   "outputs": [],
   "source": [
    "Local Response Normalization (LRN) type of layer turns out to be useful when using neurons with unbounded activations (e.g. rectified linear neurons), because it permits the detection of high-frequency features with a big neuron response, while damping responses that are uniformly large in a local neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add80960",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weight regularization does not seem widely used in CNN models, or if it is used, its use is not widely reported. L2 weight regularization with very small regularization hyperparameters such as (e.g. 0.0005 or 5 x 10^−4) may be a good starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98058c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGNet is invented by Visual Geometry Group (by Oxford University). This architecture is the 1st runner up of ILSVR2014 in the classification task while the winner is GoogLeNet. The reason to understand VGGNet is that many modern image classification models are built on top of this architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0663cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Their architecture consisted of a 22 layer deep CNN but reduced the number of parameters from 60 million (AlexNet) to 4 million. The runner-up at the ILSVRC 2014 competition is dubbed VGGNet by the community and was developed by Simonyan and Zisserman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ba737",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 regularization and L2 regularization are 2 popular regularization techniques we could use to combat the overfitting in our model. Possibly due to the similar names, it’s very easy to think of L1 and L2 regularization as being the same, especially since they both prevent overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
